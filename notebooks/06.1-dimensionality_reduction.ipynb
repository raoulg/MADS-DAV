{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<td>\n",
    "<a href=\"https://colab.research.google.com/github/raoulg/MADS-DAV/blob/main/notebooks/06.1-dimensionality_reduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "</td>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVD and PCA\n",
    "Singular Value Decomposition, or SVD for short, is a mathematical technique that decomposes a matrix into three simpler matrices. Imagine you have a large, complex puzzle; SVD helps you break it down into smaller, easier-to-understand pieces.\n",
    "\n",
    "Single Value Decomposition (SVD) is defined like this:\n",
    "\n",
    "$$X = U \\Sigma V^T$$\n",
    "\n",
    "U (left singular vectors): This is a matrix that contains information about the patterns and relationships between the rows of the original matrix.\n",
    "\n",
    "Î£ (singular values): This is a diagonal matrix with non-negative numbers, which can be seen as the \"strength\" or \"importance\" of each pattern found by U and V. The singular values are sorted from largest to smallest, showing the ranking of the importance of each pattern.\n",
    "\n",
    "V^T (right singular vectors, transpose of V): This is a matrix that contains information about the patterns and relationships between the columns of the original matrix.\n",
    "\n",
    "What are some applications where you could encounter SVD?\n",
    "\n",
    "1. Data compression: SVD can help you reduce the size of the data without losing much information. By keeping only the largest singular values (and associated vectors), you can get a compact version of your original matrix.\n",
    "2. Noise reduction: SVD can help remove noise from data. In this context, noise is represented by the components with smaller singular values that you can discard, leaving you with the more significant, stronger signals in your data.\n",
    "3. Latent semantic analysis: In natural language processing, SVD is used to understand the relationships between documents and terms in text data, helping to find the hidden (or \"latent\") concepts.\n",
    "4. Principal Component Analysis (PCA): In machine learning, SVD is used as part of PCA, which is a method that reduces the dimensionality of data while retaining most of the variation in the dataset.\n",
    "\n",
    "SVD is like a Swiss Army knife for matrices\n",
    "\n",
    "## Explanation of some terminology\n",
    "\n",
    "$U$ provides an orthonormal basis for the column space of $X$, and V provides an orthonormal basis for the row space of $X$.\n",
    "\n",
    "Some explanation of what an orthonormal basis means in simple terms:\n",
    "\n",
    "- *Orthogonal*: Each line is at a 90-degree angle to the others, like the corners of a room. One line goes from wall to wall, another from floor to ceiling, and another from one corner of the room to the opposite corner if it's a 3D space. They don't lean toward each other at all. This is how we typically think of axis lines in a graph. Note that they don't NEED to be at a 90 degree angle, but it doesnt make much sense to have them at any other angle.\n",
    "\n",
    "- *Normalized*: Each line has been stretched or shrunk to exactly the same length. Let's say we've decided that the length is the length of a meter stick. So, if you were to walk from the center of the room to the wall following one of these lines, you'd always walk exactly one meter, no matter which line you chose.\n",
    "\n",
    "- *Basis*: Using these lines, you can reach any point in the room by walking along them one at a time. In a grid with 3D axis, the basis would be the vectors (0,0,1), (0,1,0) and (1,0,0). If you want to reach any point in the room, you can reach it by combining these three vectors.\n",
    "\n",
    "The key characteristics of a basis are:\n",
    "- *Coverage*: A basis must be able to reach or \"span\" every point in the space. Just like with our north and east directions on the floor, by going some amount north and some amount east, you can reach any point on the floor. If you were in a three-dimensional room, you'd also need an up-and-down direction to reach every point.\n",
    "\n",
    "- *Independence*: The directions in a basis must be independent of each other, meaning you can't create one direction just by using a combination of the others. On our floor, you can't get a north direction just by walking east or vice versa; they're completely separate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an illustration of how minimizing the distance of the red dotted line is the same as maximizing the green dotted line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the line L (y = mx + c form)\n",
    "m, c = 1, 0  # slope and y-intercept\n",
    "x = np.linspace(-5, 5, 400)\n",
    "y = m * x + c\n",
    "\n",
    "# Define the point P\n",
    "P = np.array([2, 3])\n",
    "\n",
    "# Calculate the projection P' onto the line L\n",
    "# Line's normal vector\n",
    "normal = np.array([-m, 1])\n",
    "# Projection of P onto the line (using dot product)\n",
    "P_proj = P - np.dot(P, normal) / np.dot(normal, normal) * normal\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.plot(x, y, label=\"Line L (y = x)\", color=\"blue\")  # Line\n",
    "plt.scatter(*P, color=\"red\", label=\"Point P\")  # Point P\n",
    "plt.scatter(*P_proj, color=\"green\", label=\"Projected Point P'\")  # Projected Point P'\n",
    "plt.plot(\n",
    "    [P[0], P_proj[0]],\n",
    "    [P[1], P_proj[1]],\n",
    "    color=\"red\",\n",
    "    linestyle=\"--\",\n",
    "    label=\"Minimized Distance\",\n",
    ")  # Minimized Distance\n",
    "plt.plot(\n",
    "    [0, P_proj[0]],\n",
    "    [0, P_proj[1]],\n",
    "    color=\"green\",\n",
    "    linestyle=\"--\",\n",
    "    label=\"Maximized Distance\",\n",
    ")  # Maximized Distance\n",
    "\n",
    "# Annotations and decorations\n",
    "plt.scatter(0, 0, color=\"black\", label=\"Origin O\")  # Origin\n",
    "plt.xlabel(\"X-axis\")\n",
    "plt.ylabel(\"Y-axis\")\n",
    "plt.axhline(0, color=\"black\", linewidth=0.5)\n",
    "plt.axvline(0, color=\"black\", linewidth=0.5)\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.title(\n",
    "    \"Minimizing Distance from Point to Line Projection\\n and Maximizing Distance from Origin to Projected Point\"\n",
    ")\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets make some synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def synthetic_data(seed: int = 42, m: int = 60) -> np.array:\n",
    "    np.random.seed(seed)\n",
    "    # two weigths\n",
    "    w1, w2 = 0.1, 0.3\n",
    "    # some noise\n",
    "    noise = 0.1\n",
    "\n",
    "    # m random angles\n",
    "    angles = np.random.rand(m) * 3 * np.pi / 2 - 0.5\n",
    "    X = np.empty((m, 3))  # noqa: N806\n",
    "    X[:, 0] = np.cos(angles) + np.sin(angles) / 2 + noise * np.random.randn(m) / 2\n",
    "    X[:, 1] = np.sin(angles) * 0.7 + noise * np.random.randn(m) / 2\n",
    "    X[:, 2] = X[:, 0] * w1 + X[:, 1] * w2 + noise * np.random.randn(m)\n",
    "    return X\n",
    "\n",
    "\n",
    "X = synthetic_data(seed=4, m=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a SVD on it with numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_centered = X - X.mean(axis=0)\n",
    "U, s, Vt = np.linalg.svd(X_centered)\n",
    "U.shape, s.shape, Vt.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And lets visualize both the data, and the eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "# Scatter plot of the original data\n",
    "ax.scatter(X_centered[:, 0], X_centered[:, 1], X_centered[:, 2])\n",
    "\n",
    "# The principal components are the rows of Vt. We scale them by the square root of the eigenvalues (s**2).\n",
    "for i in range(Vt.shape[0]):\n",
    "    # Start the line in the middle of the data\n",
    "    start_point = np.zeros(3)\n",
    "    # The end of the line is the direction of the principal component\n",
    "    end_point = Vt[i, :]\n",
    "    # Plot the principal components as lines\n",
    "    ax.quiver(\n",
    "        start_point[0],\n",
    "        start_point[1],\n",
    "        start_point[2],\n",
    "        end_point[0],\n",
    "        end_point[1],\n",
    "        end_point[2],\n",
    "        color=[\"r\", \"g\", \"b\"][i],\n",
    "        arrow_length_ratio=0.05,\n",
    "        linewidths=3,\n",
    "    )\n",
    "\n",
    "# Set labels for axes\n",
    "ax.set_xlabel(\"X axis\")\n",
    "ax.set_ylabel(\"Y axis\")\n",
    "ax.set_zlabel(\"Z axis\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the data (blue points) are spread out over a diagonal surface. With SVD we are able to find three vectors that are orthogonal to each other, and that span the space of the data. They also make intuitively the most sense; the red vector covers more of the data than the X or Y axis does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take the first two eigenvectors and project the data onto them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 = Vt.T[:, :2]\n",
    "X2D_svd = X_centered.dot(W2)\n",
    "X2D_svd.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets visualize the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.scatterplot(x=X2D_svd[:, 0], y=X2D_svd[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this is a nice way to \"project\" the data onto a lower dimension (in this case, from 3D to 2D) while retaining most of the information. This is the basis of PCA.\n",
    "Now, PCA with two components should give the same result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X2D = pca.fit_transform(X)\n",
    "sns.scatterplot(x=X2D[:, 0], y=X2D[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though the y-axis is sometimes flipped, the data is the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(X2D_svd, X2D), np.allclose(X2D_svd, -X2D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look up the explained variance ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means the first variable explains 84% of the variance, the second one an additional 14%, such that just two dimensions explain 98% of the variance!\n",
    "\n",
    "This is something we can also calculate from the SVD, by taking the square of the singular values, and dividing by the sum of the squares of the singular values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.square(s) / np.sum(np.square(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Swiss roll\n",
    "Let's have a look at a synthetic dataset known as the swiss roll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_swiss_roll\n",
    "\n",
    "X, t = make_swiss_roll(n_samples=1000, noise=0.2, random_state=42)\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=t, cmap=plt.cm.hot)\n",
    "ax.view_init(10, -70)\n",
    "ax.set_title(\"Swiss Roll Dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And run PCA on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "X2D = pca.fit_transform(X)\n",
    "sns.scatterplot(x=X2D[:, 0], y=X2D[:, 1], hue=t, palette=plt.cm.hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets try t-SNE to visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "X2D = tsne.fit_transform(X)\n",
    "sns.scatterplot(x=X2D[:, 0], y=X2D[:, 1], hue=t, palette=plt.cm.hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's pretty different! But the real value of t-SNE is that it can be used to visualize high-dimensional data, where the manifold is very complex and non-linear.\n",
    "\n",
    "First, download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from pathlib import Path\n",
    "\n",
    "cache = Path.home() / \".cache/mads_datasets\"\n",
    "if not cache.exists():\n",
    "    cache.mkdir(parents=True)\n",
    "\n",
    "\n",
    "mnist = fetch_openml(\"mnist_784\", version=1, as_frame=False, data_home=str(cache))\n",
    "mnist.target = mnist.target.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = mnist[\"data\"]\n",
    "y = mnist[\"target\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have 52500 examples of handwritten digits in the traindata, each represented as a vector of 784 pixels (28x28 images flattened out), and 17500 examples in the testdata.\n",
    "\n",
    "Lets visualize the first one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = X_train[0]\n",
    "plt.imshow(img.reshape(28, 28), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is clearly an handwritten digit, and we represent it as a matrix in $\\mathbb{R}^{28 \\times 28}$, or as a vector in $\\mathbb{R}^{784}$. Now, lets try to see what happens if we visualise the data in 2D with PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "X2D = pca.fit_transform(X_train)\n",
    "sns.scatterplot(x=X2D[:, 0], y=X2D[:, 1], hue=y_train, palette=\"tab10\")\n",
    "X2D.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, well, PCA does something, but it's not very useful in two dimensions... Now lets try t-SNE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "X2D = tsne.fit_transform(X_train)\n",
    "sns.scatterplot(x=X2D[:, 0], y=X2D[:, 1], hue=y_train, palette=\"tab10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does take a lot longer, but the result is much much better for visualisation purposes!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wa-analyzer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
