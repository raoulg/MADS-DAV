{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mads_datasets import DatasetFactoryProvider, DatasetType\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "penguinsdataset = DatasetFactoryProvider.create_factory(DatasetType.PENGUINS)\n",
    "penguinsdataset.download_data()\n",
    "\n",
    "df = pd.read_parquet(penguinsdataset.filepath)\n",
    "select = [\n",
    "    \"Species\",\n",
    "    \"Island\",\n",
    "    \"Culmen Length (mm)\",\n",
    "    \"Culmen Depth (mm)\",\n",
    "    \"Flipper Length (mm)\",\n",
    "    \"Delta 15 N (o/oo)\",\n",
    "    \"Delta 13 C (o/oo)\",\n",
    "    \"Sex\",\n",
    "    \"Body Mass (g)\",\n",
    "]\n",
    "subset = df[select].dropna()\n",
    "subset[\"Species\"] = subset[\"Species\"].apply(lambda x: x.split()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we have this dataset. And we have some understanding of different types of distributions. Just from looking at the feature names it should be obvious that we can split the features into continuous and discrete features. \n",
    "\n",
    "But how do we find out which distribution fits our data best? Are these all normal distributions? Some of them, probably, but maybe not. How do we find out?\n",
    "\n",
    "## Testing for normality\n",
    "\n",
    "## qq-plot\n",
    "\n",
    "The easiest way to test for normality is to use a qq-plot. This is a very basic and visual test. It is not very precise, but it is a good first step.\n",
    "\n",
    "What is a qq-plot? It stands for quantile-quantile plot. It is a plot of the quantiles of the data against the quantiles of a theoretical distribution. If the data is normally distributed, the points will fall on a straight line. If the data is not normally distributed, the points will not fall on a straight line.\n",
    "\n",
    "First, as a reminder, let's plot the PDFs for a normal and a skew-normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare axes\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# get the pdfs for the two distributions\n",
    "x = np.linspace(-3, 3, 100)\n",
    "y1 = stats.norm.pdf(x, loc=0, scale=1)\n",
    "y2 = stats.skewnorm.pdf(x, a=5, loc=0, scale=1)  # a is the skewness parameter\n",
    "\n",
    "# plot the two distributions\n",
    "ax[0].plot(x, y1)\n",
    "ax[1].plot(x, y2)\n",
    "ax[0].set_title(\"Normal distribution\")\n",
    "ax[1].set_title(\"Skewed distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's sample some data from a distribution and see how the qq plot works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "n = 1000\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "\n",
    "# lets sample n times from a skewed normal distribution with alpha skewness\n",
    "alpha = 10\n",
    "data = stats.skewnorm.rvs(a=alpha, size=n, random_state=rng)\n",
    "\n",
    "# first, we plot the data with a theoretical normal distribution\n",
    "stats.probplot(data, plot=ax[0], dist=stats.norm)\n",
    "ax[0].set_title(\"Normal Q-Q plot\")\n",
    "\n",
    "# then, we plot the data with a theoretical skewed normal distribution\n",
    "stats.probplot(data, plot=ax[1], dist=stats.skewnorm, sparams=(alpha,))\n",
    "ax[1].set_title(\"Skew-normal Q-Q plot\")\n",
    "\n",
    "# this also works the other way around! (eg generate normal data,\n",
    "# and then the theoretical normal distribution will align,\n",
    "# but the theoretical skewed normal distribution will not align)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, for the theoretical normal distribution, the points fall on a straight line. However, for a theoretical skew-normal distribution, the points do not fall on a straight line. You can see that the theory and the data do not match, which implicates that the data is not normally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's test this for all continuous features from the penguins dataset\n",
    "\n",
    "# select float columns, just from Adelie species\n",
    "# question: what would happen if we used all species?\n",
    "adelie = subset[subset[\"Species\"] == \"Adelie\"]\n",
    "floats = adelie.select_dtypes(include=\"float64\")\n",
    "features = floats.columns\n",
    "\n",
    "# prepare axes\n",
    "fig, axs = plt.subplots(2, 3, figsize=(12, 8))\n",
    "axs = axs.ravel()\n",
    "\n",
    "for i, col in enumerate(features):\n",
    "    feats = floats[col]\n",
    "    stats.probplot(feats, plot=axs[i], dist=stats.norm)\n",
    "    axs[i].set_title(f\"{col} Q-Q plot\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, simply visualising qq-plots already gives us a pretty good idea of what distributions probably dont align. There are many reasons for this: maybe there are more groups underlying the distribution (simpsons paradox!!), and if you split the group into subgroups a normal distribution will emerge. But there could be numerous reasons. Typically, you will need domain knowledge to understand what is going on.\n",
    "\n",
    "# Fitting distributions\n",
    "\n",
    "If you have a reasonable idea what distribution your data follows, you can fit the parameters of the distribution to your data. This is called fitting a distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we generate some data. This way we can check if it works\n",
    "kwargs = {\"loc\": 5, \"scale\": 2}\n",
    "n = 100\n",
    "dist = stats.norm\n",
    "data = dist.rvs(size=n, random_state=rng, **kwargs)\n",
    "\n",
    "# we can provide fit with a range of parameters to try to fit. This will speed up things,\n",
    "# but note that there is a risk of setting the bounds too narrow and missing the true parameters!\n",
    "# or you set the bounds too wide and the fit will take a long time\n",
    "bounds = ((0, 10), (0, 4))\n",
    "result = stats.fit(dist, data, bounds=bounds, method=\"mle\")\n",
    "result.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, even with a 100 samples we get a pretty good fit. Try to play with this: change the n and see if it gets harder to fit the data. Also, change the parameters to see if the fit follows your changes. \n",
    "\n",
    "But can we know if a normal distribution is a good fit too begin with? E.g., for the penguins is seems to be the case that the Delta 13 C feature is not normally distibuted, but how sure are we of the others? \n",
    "\n",
    "We can use a [kstest](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test) to find out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use dist, which is the normal distribution from the previous cell, to create a cdf\n",
    "cdf = dist(**kwargs).cdf\n",
    "\n",
    "# now we can perform the Kolmogorov-Smirnov test on the data\n",
    "stats.kstest(data, cdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a p-value. If the p-value is larger than 0.05, we cannot reject the null hypothesis that the two distributions are similar. \n",
    "For the people that like to think ahead of the lessons: yes, we could also use a bayesian approach to test this hypothesis. But we will get to that in later lessons...\n",
    "\n",
    "Lets see what happens if we test the hypothesis that the data is a uniform distribution.\n",
    "Since the kdtest compares two cdf (cumulative distribution functions), we need to create a cdf for the uniform distribution and pass that as a second argument to the kstest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf = stats.uniform.cdf\n",
    "stats.kstest(data, cdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, that is a very small p-value. So we can reject the null hypothesis that the data is a uniform distribution.\n",
    "\n",
    "Let's automate this test for all the continuous features in the penguins dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 3, figsize=(12, 8))\n",
    "axs = axs.ravel()\n",
    "dist = stats.norm\n",
    "\n",
    "for i, col in enumerate(features):\n",
    "    data = floats[col]\n",
    "\n",
    "    # lets get a good starting point for the parameters\n",
    "    mu = data.mean()\n",
    "    sigma = data.std()\n",
    "    bounds = ((mu - 3 * sigma, mu + 3 * sigma), (0, sigma * 2))\n",
    "    result = stats.fit(dist, data, bounds=bounds)\n",
    "\n",
    "    # and plot the fitted result, including the p-values\n",
    "    result.plot(ax=axs[i])\n",
    "    kstest = stats.kstest(data, stats.norm(*result.params).cdf)\n",
    "    axs[i].set_title(f\"{col} fit (p={kstest.pvalue:.2f})\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are not a penguin expert, but can you come up with possible reasons why the body mass is a lousy fit? E.g. are there possible subgroups you can think of, that would split the distribution into two groups?\n",
    "\n",
    "It could very wll be that the subgroups turn out to be normally distributed, and we are just looking at two overlapping groups. Another reason could be that there is some other reason that influences our sample, e.g. the penguins are fed a certain diet, or are being hunted etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing if two distributions are the same\n",
    "The following code is an example to test if two samples follow the same distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def analyze_reaction_times(data1, data2, label1=\"Day Type 1\", label2=\"Day Type 2\"):\n",
    "    # 1. Test if each dataset follows an exponential distribution\n",
    "    print(f\"Sample sizes: {label1}: {len(data1)}, {label2}: {len(data2)}\")\n",
    "\n",
    "    # Fit exponential distributions to each dataset\n",
    "    loc1, scale1 = stats.expon.fit(data1)\n",
    "    loc2, scale2 = stats.expon.fit(data2)\n",
    "\n",
    "    print(\"\\nExponential fit parameters:\")\n",
    "    print(f\"{label1}: location={loc1:.4f}, scale={scale1:.4f}\")\n",
    "    print(f\"{label2}: location={loc2:.4f}, scale={scale2:.4f}\")\n",
    "\n",
    "    # Kolmogorov-Smirnov test for exponential distribution\n",
    "    ks1, p1 = stats.kstest(data1, \"expon\", args=(loc1, scale1))\n",
    "    ks2, p2 = stats.kstest(data2, \"expon\", args=(loc2, scale2))\n",
    "\n",
    "    print(\"\\nTesting if datasets follow exponential distributions:\")\n",
    "    print(\n",
    "        f\"{label1}: KS statistic={ks1:.4f}, p-value={p1:.4f} {'(follows exponential)' if p1 > 0.05 else '(does NOT follow exponential)'}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"{label2}: KS statistic={ks2:.4f}, p-value={p2:.4f} {'(follows exponential)' if p2 > 0.05 else '(does NOT follow exponential)'}\"\n",
    "    )\n",
    "\n",
    "    # 2. Compare the two distributions\n",
    "    # Method 1: Two-sample Kolmogorov-Smirnov test\n",
    "    ks_2samp, p_2samp = stats.ks_2samp(data1, data2)\n",
    "    print(f\"\\nTwo-sample KS test: statistic={ks_2samp:.4f}, p-value={p_2samp:.4f}\")\n",
    "    print(\n",
    "        f\"Conclusion: The two distributions are {'different' if p_2samp < 0.05 else 'not significantly different'}\"\n",
    "    )\n",
    "\n",
    "    # Calculate descriptive statistics\n",
    "    mean1, mean2 = np.mean(data1), np.mean(data2)\n",
    "    median1, median2 = np.median(data1), np.median(data2)\n",
    "\n",
    "    print(\"\\nDescriptive Statistics:\")\n",
    "    print(f\"{label1}: Mean={mean1:.4f}, Median={median1:.4f}\")\n",
    "    print(f\"{label2}: Mean={mean2:.4f}, Median={median2:.4f}\")\n",
    "\n",
    "    # Visual comparison\n",
    "    plt.figure(figsize=(14, 6))\n",
    "\n",
    "    # Histogram plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(data1, bins=20, alpha=0.5, density=True, label=label1)\n",
    "    plt.hist(data2, bins=20, alpha=0.5, density=True, label=label2)\n",
    "\n",
    "    # Plot fitted exponential PDFs\n",
    "    x = np.linspace(0, max(max(data1), max(data2)), 1000)\n",
    "    plt.plot(\n",
    "        x, stats.expon.pdf(x, loc=loc1, scale=scale1), \"r-\", lw=2, label=f\"{label1} fit\"\n",
    "    )\n",
    "    plt.plot(\n",
    "        x, stats.expon.pdf(x, loc=loc2, scale=scale2), \"g-\", lw=2, label=f\"{label2} fit\"\n",
    "    )\n",
    "\n",
    "    plt.title(\"Histogram with Exponential Fits\")\n",
    "    plt.xlabel(\"Reaction Time\")\n",
    "    plt.ylabel(\"Density\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Q-Q plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    stats.probplot(data1, dist=\"expon\", plot=plt)\n",
    "    plt.title(f\"Q-Q Plot for {label1}\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Another figure for the second Q-Q plot\n",
    "    plt.figure(figsize=(7, 6))\n",
    "    stats.probplot(data2, dist=\"expon\", plot=plt)\n",
    "    plt.title(f\"Q-Q Plot for {label2}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Generate sample data from two different exponential distributions\n",
    "np.random.seed(42)  # For reproducibility\n",
    "\n",
    "# Day 1: Exponential with scale=2 (mean = 2)\n",
    "reaction_times_day1 = stats.expon.rvs(scale=2, size=200)\n",
    "\n",
    "# Day 2: Exponential with scale=3 (mean = 3)\n",
    "reaction_times_day2 = stats.expon.rvs(scale=3, size=250)\n",
    "\n",
    "# Run analysis on the simulated data\n",
    "analyze_reaction_times(\n",
    "    reaction_times_day1,\n",
    "    reaction_times_day2,\n",
    "    label1=\"Day Type 1 (Scale=2)\",\n",
    "    label2=\"Day Type 2 (Scale=3)\",\n",
    ")\n",
    "\n",
    "# Let's also run a simulation with very close parameters to see the detection sensitivity\n",
    "print(\"\\n\\n\" + \"=\" * 50)\n",
    "print(\"Testing with more similar distributions:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Generate two sets with closer parameters\n",
    "similar_times_1 = stats.expon.rvs(scale=2.5, size=200)\n",
    "similar_times_2 = stats.expon.rvs(scale=2.8, size=200)\n",
    "\n",
    "analyze_reaction_times(\n",
    "    similar_times_1,\n",
    "    similar_times_2,\n",
    "    label1=\"Day Type 1 (Scale=2.5)\",\n",
    "    label2=\"Day Type 2 (Scale=2.8)\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
