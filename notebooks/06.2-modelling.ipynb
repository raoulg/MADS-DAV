{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<td>\n",
    "<a href=\"https://colab.research.google.com/github/raoulg/MADS-DAV/blob/main/notebooks/06.2-modelling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "</td>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is an implementation of the approach described in this [essay](https://www.orphanalytics.com/en/news/whitepaper202012/OrphAnalyticsQAnon2020.pdf) and written about [here](https://www.prnewswire.com/news-releases/qanon-is-two-different-people-shows-machine-learning-analysis-from-orphanalytics-301192981.html).\n",
    "\n",
    "It analyzes the posts of QAnon, and concludes that the messages are probably written by two persons instead of one.\n",
    "\n",
    "From the last link:\n",
    ">  QAnon has spread conspiracy theories to an unprecedentedly large audience. Its thousands of online messages have popularized narratives such as the existence of a child-trafficking deep state. Recently, it inspired a series of violent attacks and was listed as a terrorist threat by the FBI. The Swiss company OrphAnalytics just published an analysis of all messages posted by Q. Its patented technology aims at identifying authors of written documents. It has found two individual signals within the corpus of Q messages. Its new study contributes to revealing the origins and the persons behind one of the most impactful conspiracy theories in recent times.\n",
    "\n",
    "We will replicate their analysis with \"patented technoloy\" with a simple sklearn approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = Path.home() / \".cache/mads_datasets/qanon\"\n",
    "if not datadir.exists():\n",
    "    datadir.mkdir(parents=True)\n",
    "\n",
    "datafile = datadir / \"posts.json\"\n",
    "if not datafile.exists():\n",
    "    url = \"https://raw.githubusercontent.com/jkingsman/JSON-QAnon/main/posts.json\"\n",
    "    response = requests.get(url)\n",
    "    with datafile.open(\"wb\") as f:\n",
    "        f.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import json_normalize\n",
    "import json\n",
    "\n",
    "with datafile.open() as f:\n",
    "    df = json_normalize(json.load(f)[\"posts\"], sep=\"_\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up the timestamps\n",
    "\n",
    "The text describes they used the analysis from [this](https://www.businessinsider.com/every-qanon-message-q-drop-analyzed-2020-10?international=true&r=US&IR=T) businessinsider article to bin the timeseries.\n",
    "\n",
    "<img src=\"../img/Qanon.png\" width=450 height=400 />\n",
    "\n",
    "These dates are hardcoded in the bin_time method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"time\"] = df[\"post_metadata_time\"].apply(pd.to_datetime, unit=\"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def bin_time(time):\n",
    "    if time < datetime(2017, 12, 1):\n",
    "        return 0\n",
    "    elif time < datetime(2018, 1, 1):\n",
    "        return 1\n",
    "    elif time < datetime(2018, 8, 10):\n",
    "        return 2\n",
    "    elif time < datetime(2019, 8, 1):\n",
    "        return 3\n",
    "    else:\n",
    "        return 4\n",
    "\n",
    "df[\"bintime\"] = df[\"time\"].apply(lambda x : bin_time(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's clean up the text some more. We will replace linebreaks (\"\\n\") with spaces, remove urls and set everything to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_url(text):\n",
    "    return re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text)\n",
    "\n",
    "df[\"text\"] = df[\"text\"].apply(lambda x : str(x).replace(\"\\n\", \" \"))\n",
    "df[\"text\"] = df[\"text\"].apply(lambda x : remove_url(x))\n",
    "df[\"text\"] = df[\"text\"].apply(lambda x : x.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we count every text and filter chuncks with size below 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['size'] = df['text'].apply(lambda x : len(str(x)))\n",
    "df = df[df[\"size\"] > 50]\n",
    "df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper describes batching all the text into chunks of 7500 characters. We can obtain sort of the same by splitting the text up into 100 batches, 7.7k each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_seq(text, k):\n",
    "\tlongseq = \" \".join(text)\n",
    "\tn = int(len(longseq) / k)\n",
    "\tparts = [longseq[i:i+n] for i in range(0, len(longseq), n)]\n",
    "\treturn parts, n\n",
    "\n",
    "z=100\n",
    "parts, n = batch_seq(df[\"text\"], k=z)\n",
    "parts = parts[:-1]\n",
    "len(parts), n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, 'group'] = df[\"size\"].cumsum() // n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper uses a ngram with size 3 on characterlevel. We can do that with `CountVectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "vectorizer = CountVectorizer(analyzer='char', ngram_range=(3, 3))\n",
    "X = vectorizer.fit_transform(parts)\n",
    "X = np.asarray(X.todense())\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And calculates the pairwise manhattan distances. That way, we get an idea how how different every chunk is, when compared to all other chunks, in terms of the ngrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import manhattan_distances\n",
    "distance = manhattan_distances(X, X)\n",
    "distance.shape, type(distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To handle the labels, we make integers of the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df[\"post_metadata_source_site\"].unique()\n",
    "mapping = {labels[i]:i for i in range(3)}\n",
    "df[\"source\"] = df[\"post_metadata_source_site\"].apply(lambda x : mapping[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "model = pca.fit_transform(distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sourcemean = np.round(df.groupby(\"group\").source.mean()).values.reshape(-1,1)\n",
    "timemean = np.round(df.groupby(\"group\").bintime.mean()).values.reshape(-1,1)\n",
    "p = pd.DataFrame(np.concatenate([model, sourcemean, timemean], axis=1), columns=[\"x\", \"y\", \"source\", \"time\"]).reset_index()\n",
    "sns.scatterplot(data= p, x=\"x\", y=\"y\", hue=\"source\", style=\"source\", palette=\"Set1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This reproduces the main conclusion, that the red source is not overlapping with the other two sources.\n",
    "\n",
    "Of course, this tells you nothing about how high the chance is, that this is actually caused by having two authors, or by somehow changing the writing style.\n",
    "\n",
    "For that, we would need access to how likely it is that the writing style changes this much.\n",
    "\n",
    "However, we still have reproduced their \"patented technology\" with a few lines of code.\n",
    "Let's wrap it into a class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"..\")\n",
    "\n",
    "from src.model import TextClustering\n",
    "\n",
    "clustering = TextClustering()\n",
    "labels = sourcemean.flatten().astype(str)\n",
    "clustering(text=df[\"text\"], k=100, labels=labels, batch=True, method=\"PCA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now lets test it on the whatsapp data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed = Path(\"../data/processed\")\n",
    "datafile = processed / \"whatsapp-20240122-182706.parq\"\n",
    "if not datafile.exists():\n",
    "    logger.warning(\"Datafile does not exist. First run src/preprocess.py, and check the timestamp!\")\n",
    "wa_df = pd.read_parquet(datafile)\n",
    "wa_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know the authors, so let's use the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = list(np.unique(wa_df.author))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take a subset of every author, and join their texts.\n",
    "Obviously, you can only do this if you know the authors, or at least the claimed authors. Concatenating all texts together will make the data useless. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2000\n",
    "min_parts = 2\n",
    "\n",
    "corpus = {}\n",
    "for author in authors:\n",
    "\tsubset = wa_df[wa_df.author == author].reset_index()\n",
    "\tlongseq = \" \".join(subset.message)\n",
    "\t# chunk everything into n-sized parts\n",
    "\tparts = [longseq[i:i+n] for i in range(0, len(longseq), n)]\n",
    "\t# clean urls\n",
    "\tparts = [remove_url(chunk) for chunk in parts]\n",
    "\t# remove double spaces\n",
    "\tparts = [re.sub(' +', ' ', chunk) for chunk in parts]\n",
    "\t# keep only parts with more than min_parts\n",
    "\tif len(parts) > min_parts:\n",
    "\t\tcorpus[author] = parts\n",
    "corpus.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "text = [part for text in corpus.values() for part in text]\n",
    "wa_labels = [k for k,v in corpus.items() for _ in range(len(v))]\n",
    "# we set batch to false, because we already batched the data\n",
    "clustering(text=text, k=100, labels=wa_labels, batch=False, method=\"tSNE\")\n",
    "plt.legend(title='Author', bbox_to_anchor=(1.05, 1), loc='upper left')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
