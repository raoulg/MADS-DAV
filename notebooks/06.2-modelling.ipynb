{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<td>\n",
    "<a href=\"https://colab.research.google.com/github/raoulg/MADS-DAV/blob/main/notebooks/06.2-modelling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "</td>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from loguru import logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is an implementation of the approach described in this [essay](https://www.orphanalytics.com/en/news/whitepaper202012/OrphAnalyticsQAnon2020.pdf) and written about [here](https://www.prnewswire.com/news-releases/qanon-is-two-different-people-shows-machine-learning-analysis-from-orphanalytics-301192981.html).\n",
    "\n",
    "It analyzes the posts of QAnon, and concludes that the messages are probably written by two persons instead of one.\n",
    "\n",
    "From the last link:\n",
    ">  QAnon has spread conspiracy theories to an unprecedentedly large audience. Its thousands of online messages have popularized narratives such as the existence of a child-trafficking deep state. Recently, it inspired a series of violent attacks and was listed as a terrorist threat by the FBI. The Swiss company OrphAnalytics just published an analysis of all messages posted by Q. Its patented technology aims at identifying authors of written documents. It has found two individual signals within the corpus of Q messages. Its new study contributes to revealing the origins and the persons behind one of the most impactful conspiracy theories in recent times.\n",
    "\n",
    "We will replicate their analysis with \"patented technoloy\" with a simple sklearn approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = Path.home() / \".cache/mads_datasets/qanon\"\n",
    "if not datadir.exists():\n",
    "    datadir.mkdir(parents=True)\n",
    "\n",
    "datafile = datadir / \"posts.json\"\n",
    "if not datafile.exists():\n",
    "    logger.info(\"Downloading QAnon data\")\n",
    "    url = \"https://raw.githubusercontent.com/jkingsman/JSON-QAnon/main/posts.json\"\n",
    "    response = requests.get(url, timeout=10)\n",
    "    with datafile.open(\"wb\") as f:\n",
    "        f.write(response.content)\n",
    "else:\n",
    "    logger.info(f\"QAnon data already downloaded at {datafile}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On Linux and macOS, the default encoding is typically UTF-8.\n",
    "On Windows, the default encoding is not UTF-8 but rather a locale-specific encoding, often CP1252 or similar. Because, why not use your own proprietary encoding, just to be backwards compatible, even though the rest of the world uses something different?\n",
    "\n",
    "If you are running this code on a windows machine, you will need to specify the encoding as 'utf-8', like I did below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import json_normalize\n",
    "import json\n",
    "\n",
    "with datafile.open(encoding=\"utf-8\") as f:\n",
    "    df = json_normalize(json.load(f)[\"posts\"], sep=\"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up the timestamps\n",
    "\n",
    "The text describes they used the analysis from [this](https://www.businessinsider.com/every-qanon-message-q-drop-analyzed-2020-10?international=true&r=US&IR=T) businessinsider article to bin the timeseries.\n",
    "\n",
    "<img src=\"../img/Qanon.png\" width=450 height=400 />\n",
    "\n",
    "These dates are hardcoded in the bin_time method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"time\"] = df[\"post_metadata_time\"].apply(pd.to_datetime, unit=\"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def bin_time(time):\n",
    "    if time < datetime(2017, 12, 1):\n",
    "        return 0\n",
    "    elif time < datetime(2018, 1, 6):\n",
    "        return 1\n",
    "    elif time < datetime(2018, 8, 10):\n",
    "        return 2\n",
    "    elif time < datetime(2019, 8, 1):\n",
    "        return 3\n",
    "    else:\n",
    "        return 4\n",
    "\n",
    "\n",
    "df[\"bintime\"] = df[\"time\"].apply(lambda x: bin_time(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timezone\n",
    "datetime(2017, 12, 1, tzinfo=timezone.utc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"bintime\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's clean up the text some more. We will replace linebreaks (\"\\n\") with spaces, remove urls and set everything to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def remove_url(text):\n",
    "    return re.sub(r\"^https?:\\/\\/.*[\\r\\n]*\", \"\", text)\n",
    "\n",
    "\n",
    "df[\"text\"] = df[\"text\"].apply(lambda x: str(x).replace(\"\\n\", \" \"))\n",
    "df[\"text\"] = df[\"text\"].apply(lambda x: remove_url(x))\n",
    "df[\"text\"] = df[\"text\"].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we count every text and filter chuncks with size below 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"size\"] = df[\"text\"].apply(lambda x: len(str(x)))\n",
    "df = df[df[\"size\"] > 50]\n",
    "df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper describes batching all the text into chunks of 7500 characters. We can obtain sort of the same by splitting the text up into 100 batches, 7.7k each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_seq(text, k):\n",
    "    longseq = \" \".join(text)\n",
    "    n = int(len(longseq) / k)\n",
    "    parts = [longseq[i : i + n] for i in range(0, len(longseq), n)]\n",
    "    return parts, n\n",
    "\n",
    "\n",
    "z = 100\n",
    "parts, n = batch_seq(df[\"text\"], k=z)\n",
    "parts = parts[:-1]\n",
    "len(parts), n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This gives us groups of 100 posts\n",
    "df[\"size\"].cumsum() // n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets store that group in the dataframe\n",
    "df.loc[:, \"group\"] = df[\"size\"].cumsum() // n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper uses a ngram with size 3 on characterlevel. We can do that with `CountVectorizer`\n",
    "\n",
    "How does this work? We will get a matrix with 100 rows (one for every group) and many columns. What are these columns? This are the trigram-counts. Let's see how this works: for example, if I have the string \"yellow banana\", we get the trigrams: \" y\", \"ye\", \"el\", \"ll\", \"lo\", \"ow\", \"w \", \" b\", \"ba\", \"an\", \"na\", \"an\", \"na\". We can see that the trigram \"an\" appears twice. So, the column \"an\" will have the value 2, the other trigrams have the value 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "example = [\"yellow banana\", \"papagena papaya banana\"]\n",
    "vectorizer = CountVectorizer(analyzer=\"char\", ngram_range=(3, 3))\n",
    "trigram_example = vectorizer.fit_transform(example)\n",
    "logger.info(f\"The trigrams are: {vectorizer.get_feature_names_out()}\")\n",
    "logger.info(f\"Shape of the trigram matrix: {trigram_example.shape}\")\n",
    "trigram_example.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we see the model counted 23 different trigrams.\n",
    "\n",
    "Now let's proceed to do the same with our text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer=\"char\", ngram_range=(3, 3))\n",
    "X = vectorizer.fit_transform(parts)\n",
    "X = np.asarray(X.todense())\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see we found 23074 different trigrams in our text.\n",
    "\n",
    "We can now calculates the pairwise manhattan distances. That way, we get an idea how how different every chunk is, when compared to all other chunks, in terms of the ngrams. If the vectors are similar, the distance is zero, if they are very different, the distance is high.\n",
    "\n",
    "This will give us a 100x100 matrix with the distances, for every vector the distance to all other vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import manhattan_distances\n",
    "\n",
    "distance = manhattan_distances(X, X)\n",
    "distance.shape, type(distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check how this works for the trigram example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manhattan_distances(trigram_example.toarray(), trigram_example.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that the manhattan distance between the trigram vectors of \"yellow banana\" and \"papagena papaya banana\" is 21.\n",
    "To handle the labels, we make integers of the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df[\"post_metadata_source_site\"].unique()\n",
    "mapping = {labels[i]: i for i in range(3)}\n",
    "df[\"source\"] = df[\"post_metadata_source_site\"].apply(lambda x: mapping[x])\n",
    "source_names = {v: k for k, v in mapping.items()}\n",
    "source_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "model = pca.fit_transform(distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sourcemean = np.round(df.groupby(\"group\").source.mean()).values.reshape(-1, 1)\n",
    "timemean = np.round(df.groupby(\"group\").bintime.mean()).values.reshape(-1, 1)\n",
    "p = pd.DataFrame(\n",
    "    np.concatenate([model, sourcemean, timemean], axis=1),\n",
    "    columns=[\"x\", \"y\", \"source\", \"time\"],\n",
    ").reset_index()\n",
    "p[\"source\"] = p[\"source\"].map(source_names)\n",
    "sns.scatterplot(data=p, x=\"x\", y=\"y\", hue=\"source\", style=\"source\", palette=\"Set1\")\n",
    "plt.yticks([])\n",
    "plt.xticks([])\n",
    "plt.title(\"QAnon posts show two distinct authors\")\n",
    "plt.legend(title=\"Forum source\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This reproduces the main conclusion, that the red source is not overlapping with the other two sources.\n",
    "\n",
    "Of course, this tells you nothing about how high the chance is, that this is actually caused by having two authors, or by somehow changing the writing style.\n",
    "\n",
    "For that, we would need access to how likely it is that the writing style changes this much.\n",
    "\n",
    "However, we still have reproduced their \"patented technology\" with a few lines of code.\n",
    "Let's wrap it into a class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"..\")\n",
    "\n",
    "from wa_analyzer.model import TextClustering\n",
    "\n",
    "clustering = TextClustering()\n",
    "labels = sourcemean.flatten().astype(str)\n",
    "clustering(text=df[\"text\"], k=100, labels=labels, batch=True, method=\"PCA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now lets test it on the whatsapp data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tomllib\n",
    "\n",
    "configfile = Path(\"../config.toml\").resolve()\n",
    "with configfile.open(\"rb\") as f:\n",
    "    config = tomllib.load(f)\n",
    "datafile = (Path(\"..\") / Path(config[\"processed\"]) / config[\"current\"]).resolve()\n",
    "if not datafile.exists():\n",
    "    logger.warning(\n",
    "        \"Datafile does not exist. First run src/preprocess.py, and check the timestamp!\"\n",
    "    )\n",
    "wa_df = pd.read_parquet(datafile)\n",
    "wa_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know the authors, so let's use the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = list(np.unique(wa_df.author))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take a subset of every author, and join their texts.\n",
    "Obviously, you can only do this if you know the authors, or at least the claimed authors. Concatenating all texts together will make the data useless. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 500\n",
    "min_parts = 2\n",
    "\n",
    "corpus = {}\n",
    "for author in authors:\n",
    "    subset = wa_df[wa_df.author == author].reset_index()\n",
    "    longseq = \" \".join(subset.message)\n",
    "    # chunk everything into n-sized parts\n",
    "    parts = [longseq[i : i + n] for i in range(0, len(longseq), n)]\n",
    "    # clean urls\n",
    "    parts = [remove_url(chunk) for chunk in parts]\n",
    "    # remove double spaces\n",
    "    parts = [re.sub(\" +\", \" \", chunk) for chunk in parts]\n",
    "    # keep only parts with more than min_parts\n",
    "    if len(parts) > min_parts:\n",
    "        corpus[author] = parts\n",
    "corpus.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "text = [part for text in corpus.values() for part in text]\n",
    "wa_labels = [k for k, v in corpus.items() for _ in range(len(v))]\n",
    "# we set batch to false, because we already batched the data\n",
    "clustering(text=text, k=100, labels=wa_labels, batch=False, method=\"tSNE\")\n",
    "plt.legend(title=\"Author\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.title(\"Distinct authors in the WhatsApp dataset\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wa-analyzer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
