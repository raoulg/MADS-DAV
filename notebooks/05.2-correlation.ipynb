{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mads_datasets import DatasetFactoryProvider, DatasetType\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "\n",
    "penguinsdataset = DatasetFactoryProvider.create_factory(DatasetType.PENGUINS)\n",
    "penguinsdataset.download_data()\n",
    "\n",
    "df = pd.read_parquet(penguinsdataset.filepath)\n",
    "select = [\n",
    "    \"Species\",\n",
    "    \"Island\",\n",
    "    \"Culmen Length (mm)\",\n",
    "    \"Culmen Depth (mm)\",\n",
    "    \"Flipper Length (mm)\",\n",
    "    \"Delta 15 N (o/oo)\",\n",
    "    \"Delta 13 C (o/oo)\",\n",
    "    \"Sex\",\n",
    "    \"Body Mass (g)\",\n",
    "]\n",
    "subset = df[select].dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation\n",
    "\n",
    "Correlation is a measure of the relationship between two variables. The most common measure of correlation in statistics is the Pearson correlation coefficient, which is a measure of the linear relationship between two variables.\n",
    "\n",
    "The idea is this: we have been looking at variance, which will compare the difference from every individual datapoint with the mean by calculating $(x_i - \\bar{x})^2$ and taking the average of those differences. Now, what if we compare these differences between two variables? That is what the covariance does:\n",
    "\n",
    "$$cov(x,y) = \\frac{1}{n} \\sum_i (x_i - \\bar{x})(y_i - \\bar{y})$$\n",
    "\n",
    "Now, this is a bit hard to interpret, because it depends on the units of the variables. So, we can normalize this by dividing by the standard deviations of the variables:\n",
    "\n",
    "$$ r = \\frac{cov(x,y)}{\\sigma_x \\sigma_y}$$\n",
    "\n",
    "where $\\sigma$ is the standard deviation. \n",
    "\n",
    "\n",
    "or, if you want to correct for bias:\n",
    "\n",
    "$$r = \\frac{cov(x,y)}{(n-1)\\sigma_x \\sigma_y}$$\n",
    "\n",
    "This is the Pearson correlation coefficient. It is a number between -1 and 1, where 1 is a perfect positive correlation, 0 is no correlation, and -1 is a perfect negative correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "floats = subset.select_dtypes(include=\"float64\")\n",
    "correlation_matrix = floats.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(correlation_matrix, vmin=-1, vmax=1, annot=True, cmap=\"coolwarm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding correlation through regularization\n",
    "We will start with an unfortunate dataset. We have 500 datapoints, and 100 features. But there is a lot of noise, and a lot of features arent even correlated to the target!\n",
    "\n",
    "In real life situations, this might happen more often than you like; you get an abundance of features, and you have no idea what is correlated, and what is just noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "datafile = Path(\"../data/sim/correlation.csv\")\n",
    "df = pd.read_csv(datafile)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might think, lets start with plotting the correlations. Because we have a 100 features, this is too much for a heatmap, so we will limit ourselves to the correlation between the features and the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = df.corr()\n",
    "sns.scatterplot(x=correlation_matrix.index, y=correlation_matrix.target)\n",
    "plt.xticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target has obviously a correlation of 1 with itself. But the other features are much more noisy. 6 features have a correlation above 0.2, but that is still not very high.\n",
    "Maybe the two features with a correlation above 0.5 are worth looking at, but the rest might be noise if you use this method. Because this is synthetic data that I created myself, I know for a fact that 10 out of 100 features are relevant. So, for this case plain correlation is not very good at finding the relevant features.\n",
    "\n",
    "However, there are smarter ways of figuring out a correlation. First, let's split the data into features X and target y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=[\"target\"]).values\n",
    "y = df[\"target\"].values\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (mis)using a linear regression for feature selection\n",
    "What we will do, is we will construct a linear regression for this problem. \n",
    "\n",
    "Note that in this strategy, our goal is NOT to create a linear regression model. Well, we might want to do that, but that is not the point here. We are trying to get a better grip on correlation, and which features contain relevant information. We might already have decided we are going to use a random forest, or a neural network, or whatever fits our goal, or we might not have decided yet. In using a linear regression model, we are going to use regularization, and we are going to read out the weights of the model.\n",
    "\n",
    "These weights is what we are actually interested in, because it will tell us something about how informative the models are. If a weight is zero, it means that the feature is not used in the model. If a weight is high, it means that the feature is used a lot in the model. If a weight is negative, it means that the feature is used inversely in the model.\n",
    "\n",
    "This means we are going to assume there is a relation like this:\n",
    "\n",
    "$$y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_{100} x_{100}$$\n",
    "\n",
    "Where $\\beta$ are the weights we want to learn.\n",
    "\n",
    "We are going to define a \"loss\" function. This function tells us how good or bad our model is working. The better the model, the lower the loss. The worse the model, the higher the loss. The mean squared error is a common loss function, that calculates the average of the squared differences between the predictions and the actual values:\n",
    "\n",
    "$$L = \\frac{1}{n} \\sum_i (y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "# Using regularization\n",
    "But a 100 features is a lot, especially compared to 500 data points. So we are going to restrain the weights by adding a penalty to the loss function. This is called regularization. \n",
    "\n",
    "## L2 regularization\n",
    "Ridge regression (often called \"l2\") looks like this:\n",
    "\n",
    "$L = \\frac{1}{n} \\sum_i (y_i - \\hat{y}_i)^2 + \\alpha \\frac{1}{2} \\sum_j \\beta_j^2$\n",
    "\n",
    "\n",
    "The first part is the normal loss function, the second part $\\alpha \\frac{1}{2} \\sum_j \\beta_j^2$ is the regularization.\n",
    "The $\\frac{1}{2}$ helps with taking the gradient of the function: as you might remember from school when you were 16, the derivative of $a^2$ is $2a$, but the derivartive of \n",
    "$\\frac{1}{2}a^2$ is $\\frac{1}{2} 2 a$, which is simplified to just $a$ which is convenient.\n",
    "\n",
    "What we actually do is adding the square of every weight as a penalty.\n",
    "\n",
    "Can you understand what this does? The weights are an additional term that we try to minimize.\n",
    "If two models are almost equally good, but one of them has lower weights, this model will be preferred.\n",
    "$\\alpha$ is a hyperparameter that controls the strength of the regularization. The higher $\\alpha$, the more the weights will be restrained.\n",
    "\n",
    "\n",
    "## L1 regularization\n",
    "Another way of regularization is Lasso (often called \"l1\"):\n",
    "\n",
    "$$L = \\frac{1}{n} \\sum_i (y_i - \\hat{y}_i)^2 + \\alpha \\sum_j |\\beta_j|$$\n",
    "\n",
    "You notice the same mean square error loss function $\\frac{1}{n} \\sum_i (y_i - \\hat{y}_i)^2$, but now we are adding the summed ($\\Sigma$) absolute value (the $||$ mean absolute value. $|x|$ is always positive, even if $x$ is negative) of the weights $\\beta$ as a penalty, where $\\alpha$ is a parameter that determines if this part has a lot of impact (when $\\alpha$ is big) or almost no impact (when $\\alpha$ is small): $\\alpha \\sum_j |\\beta_j|$\n",
    "\n",
    "## Elasticnet\n",
    "The difference between l1 and l2 is that l1 will tend to generate more sparse weights. That is why l1 if often used for feature selection. Another option is to simply mix the two strategies, and search for a parameter that balances the two. This combination is called ElasticNet.\n",
    "\n",
    "$$L = \\frac{1}{n} \\sum_i (y_i - \\hat{y}_i)^2 + r\\alpha \\sum_j |\\beta_j| + \\alpha \\frac{1-r}{2} \\sum_j \\beta_j^2$$\n",
    "\n",
    "This formula looks impressive, but lets break it down:\n",
    "\n",
    "- the first part is the usual loss function $\\frac{1}{n} \\sum_i (y_i - \\hat{y}_i)^2$\n",
    "- the second part is the l1 regularization $\\alpha \\sum_j |\\beta_j|$. If $r$ is zero, this is removed from the equation.\n",
    "- the third part is the l2 regularization $\\alpha \\frac{1-r}{2} \\sum_j \\beta_j^2$. If $r$ is one, this is removed from the equation. \n",
    "\n",
    "Now, we can pick a value between 0 and 1 for r, and this will balance the two regularization strategies.\n",
    "\n",
    "First, let's use it with default values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "regressor = SGDRegressor(penalty=\"elasticnet\", random_state=42)\n",
    "regressor.fit(X, y)\n",
    "coef = regressor.coef_\n",
    "plt.plot(coef.T, \"o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think this is an impressive improvement over using simple correlation!\n",
    "\n",
    "We have the option to try different values for $r$ and $\\alpha$. While creating models and hypertuning is a topic for the next semester, I will show it here so you already have a look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "l1_ratio = [0.1, 0.15, 0.3, 0.6, 0.75, 0.9, 0.95, 0.99, 1]\n",
    "alphalist = [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "\n",
    "regressor = SGDRegressor(penalty=\"elasticnet\", random_state=42)\n",
    "param_grid = {\"alpha\":alphalist, \"l1_ratio\":l1_ratio}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    regressor,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    ")\n",
    "\n",
    "\n",
    "grid.fit(X, y)\n",
    "best_model = grid.best_estimator_\n",
    "coef = best_model.coef_\n",
    "plt.plot(coef.T, \"o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the result, you can see that the grid picked `l1_ratio = 1`, which means that the search found the L1 regularisation to be the best one for this problem.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wa-analyzer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
